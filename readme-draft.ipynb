{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme Draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Assembly Capstone Project: Dog Toy Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "- Project Directory \n",
    "- Problem Statement\n",
    "- Executive Summary \n",
    "- Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Directory \n",
    "INSERT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off the reviews, descriptions and key benefits of dog toys, and a person's description of their dog's general personality, I will create a recommendation system that will suggest ten dog toys that the dog might enjoy based off the similarity in what was written and the text provided about each toy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Collection** \n",
    " - Using Selenium and BeautifulSoup, scraped dog toys from [Chewy.com](https://www.chewy.com) The categories of toys I scraped are chew toys, plush toys, fetch toys, interactive toys and rope and tug toys. \n",
    " - The features that I scraped include: title, price, overall rating, description, key benefits (if this section exists), and reviews as well as the link to the toy's webpage. \n",
    " - This process was definitely the greatest challenge due to overloading the website and computer capacity. I had to make sure to take it step by step in order to ensure that I did not overwhelm Chewy as well as my computer. \n",
    " - Cleaned unneccessary characters from each column. For example, the \"\\$\" in the price and some html characters that got collected in the scraping process by accident. \n",
    " - Since I scraped toys by each category, I had to combine them all into one large dataframe to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing & EDA**  \n",
    "Preprocessing:\n",
    " - I created a new column, combining the text from the toys' description, key benefits (if present) and reviews which would then be used in analyzing the toys. \n",
    " - I then tokenized this column and removed stop words and other words I thought would not hold much meaning such as various forms of 'dog' and 'toy' since all the text was talking about dog toys. \n",
    " - I used spaCy to vectorize and then manually calculated the average vector for each toy's text for EDA. \n",
    "\n",
    "EDA: **ADD IN SOME FINDINGS**\n",
    " - Looked at the count of toys in each category and subcategory. This showed me that for some reason, there was only one toy in the ball subcategory and that I would have to rescrape data once more toys were added back to the website. \n",
    " - I also used CountVectorization to look at most commons words amongst the whole dataframe and then among each category to see if there were any words that were or were not common among all categories. \n",
    " - I also looked at the overall rating and how they were distributed amongst each category and how those categories compared to each other. \n",
    " - Lastly, I looked at how the vectors were distributed in the whole dataframe and then by category to again see if there were any major differences in the text of each category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation System** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
